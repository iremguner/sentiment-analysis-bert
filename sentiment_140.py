# -*- coding: utf-8 -*-
"""sentiment_140.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U1X5jK2YBjBdXAsCPTrrPFtrz7SMtJK6
"""

import torch
import pandas as pd
from tqdm import tqdm
import os
import re
import pandas as pd
import os
from collections import Counter
import re
from string import punctuation, digits
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import model_selection, naive_bayes
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report
from sklearn import svm
from sklearn.multiclass import OneVsRestClassifier
from sklearn.multiclass import OneVsOneClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score
from sklearn.calibration import CalibratedClassifierCV

'''
target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)
ids: The id of the tweet ( 2087)
date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)
flag: The query (lyx). If there is no query, then this value is NO_QUERY.
user: the user that tweeted (robotickilldozr)
text: the text of the tweet (Lyx is cool)
'''
data_path = 'sample_data/training.1600000.processed.noemoticon.csv'
df = pd.read_csv(data_path, encoding='ISO-8859-1', names = ['target', 'ids', 'date', 'flag', 'user', 'text'])

df.text = df.text.map(lambda x: re.sub('[^a-z0-9]+', ' ', x))  # clean text
df.text = df.text.map(lambda x: re.sub('[0-9]+', '', x))  # clean text
df.text = df.text.map(lambda x: ' '.join(word for word in x.split() if len(word) > 1 or word.isnumeric() == True))

possible_labels = df.target.unique()
label_dict = {labels:index for index,labels in enumerate(possible_labels)}
len(label_dict)

df['target'] = df.target.replace(label_dict)

df1 = df.sample(n=5000)

df1.set_index('ids', inplace = True)

df1.target.value_counts()

from sklearn.model_selection import  train_test_split

x_train , x_val, y_train, y_val = train_test_split(
    df1.index.values,
    df1.target.values,
    test_size = 0.15,
    random_state = 17,
    stratify = df1.target.values)

df1['data_type'] = ['not_set'] * df1.shape[0]
df1.loc[x_train, 'data_type'] = 'train'
df1.loc[x_val, 'data_type'] = 'val'

pip install transformers

from transformers import BertTokenizer
from torch.utils.data import TensorDataset

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', 
                                          do_lower_case=True)

encoded_data_train = tokenizer.batch_encode_plus(df1[df1.data_type == 'train'].text.values,
                                                 add_special_tokens = True,
                                                 return_attention_mask = True,
                                                 pad_to_max_length = True,
                                                 max_length = 256,
                                                 return_tensors = 'pt')

encoded_data_val = tokenizer.batch_encode_plus( df1[df1.data_type == 'val'].text.values,
                                            add_special_tokens = True,
                                            return_attention_mask = True,
                                            pad_to_max_length = True,
                                            max_length = 256,
                                            return_tensors = 'pt')

input_ids_train = encoded_data_train['input_ids']
attention_masks_train = encoded_data_train['attention_mask']
labels_train = torch.tensor(df1[df1.data_type == 'train'].target.values)

input_ids_val = encoded_data_val['input_ids']
attention_masks_val = encoded_data_val['attention_mask']
labels_val = torch.tensor(df1[df1.data_type == 'val'].target.values)

dataset_train = TensorDataset(input_ids_train, attention_masks_train,labels_train)
dataset_val = TensorDataset(input_ids_val, attention_masks_val,labels_val)

len(dataset_train)
len(dataset_val)

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(label_dict),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 8

dataloader_train = DataLoader(dataset_train,
                              sampler=RandomSampler(dataset_train),
                              batch_size=batch_size)

dataloader_validation = DataLoader(dataset_val,
                                   sampler=SequentialSampler(dataset_val),
                                   batch_size=batch_size)

from transformers import AdamW, get_linear_schedule_with_warmup

optimizer = AdamW(model.parameters(),
                  lr=1e-5,
                  eps=1e-8)

epochs = 3

scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=0,
                                            num_training_steps=len(dataloader_train)*epochs)

import numpy as np
from sklearn.metrics import f1_score


def f1_score_func(preds, labels):
	preds_flat = np.argmax(preds, axis=1).flatten()
	labels_flat = labels.flatten()
	return f1_score(labels_flat, preds_flat, average='weighted')

import random

seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

print(device)

def evaluate(dataloader_val):
	model.eval()

	loss_val_total = 0
	predictions, true_vals = [], []

	for batch in dataloader_val:
		batch = tuple(b.to(device) for b in batch)

		inputs = {'input_ids': batch[0],
				  'attention_mask': batch[1],
				  'labels': batch[2],
				  }

		with torch.no_grad():
			outputs = model(**inputs)

		loss = outputs[0]
		logits = outputs[1]
		loss_val_total += loss.item()

		logits = logits.detach().cpu().numpy()
		label_ids = inputs['labels'].cpu().numpy()
		predictions.append(logits)
		true_vals.append(label_ids)

	loss_val_avg = loss_val_total / len(dataloader_val)

	predictions = np.concatenate(predictions, axis=0)
	true_vals = np.concatenate(true_vals, axis=0)

	return loss_val_avg, predictions, true_vals

for epoch in tqdm(range(1, epochs + 1)):

	model.train()

	loss_train_total = 0

	progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
	for batch in progress_bar:
		model.zero_grad()

		batch = tuple(b.to(device) for b in batch)

		inputs = {'input_ids': batch[0],
				  'attention_mask': batch[1],
				  'labels': batch[2],
				  }

		outputs = model(**inputs)

		loss = outputs[0]
		loss_train_total += loss.item()
		loss.backward()

		torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

		optimizer.step()
		scheduler.step()

		progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})

	torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')

	tqdm.write(f'\nEpoch {epoch}')

	loss_train_avg = loss_train_total / len(dataloader_train)
	tqdm.write(f'Training loss: {loss_train_avg}')

	val_loss, predictions, true_vals = evaluate(dataloader_validation)
	val_f1 = f1_score_func(predictions, true_vals)
	tqdm.write(f'Validation loss: {val_loss}')
	tqdm.write(f'F1 Score (Weighted): {val_f1}')

model = BertForSequenceClassification.from_pretrained("bert-base-uncased",
                                                      num_labels=len(label_dict),
                                                      output_attentions=False,
                                                      output_hidden_states=False)

model.to(device)

model.load_state_dict(torch.load('/content/finetuned_BERT_epoch_2.model', map_location=torch.device('cpu')))

_, predictions, true_vals = evaluate(dataloader_validation)

y_pred  = np.argmax(predictions, axis=1).flatten()
y_true  = true_vals.flatten()

from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred))